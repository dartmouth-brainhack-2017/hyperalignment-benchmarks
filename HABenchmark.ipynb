{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mvpa2.suite import *\n",
    "verbose.level = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path to tutorial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading data...\n"
     ]
    }
   ],
   "source": [
    "# put your filepath in here!\n",
    "filepath = '/Users/vassiki/Desktop/brainhack'\n",
    "\n",
    "verbose(1, \"Loading data...\")\n",
    "filename = 'hyperalignment_tutorial_data_2.4.hdf5.gz'\n",
    "fn = os.path.join(filepath,filename)\n",
    "ds_all = h5load(fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and describe data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 subjects\n",
      "  Per-subject dataset: 56 samples with 3509 features\n",
      "  Stimulus categories: Chair, DogFace, FemaleFace, House, MaleFace, MonkeyFace, Shoe\n"
     ]
    }
   ],
   "source": [
    "# zscore all datasets individually\n",
    "_ = [zscore(ds) for ds in ds_all]\n",
    "# inject the subject ID into all datasets\n",
    "for i, sd in enumerate(ds_all):\n",
    "    sd.sa['subject'] = np.repeat(i, len(sd))\n",
    "# number of subjects\n",
    "nsubjs = len(ds_all)\n",
    "# number of categories\n",
    "ncats = len(ds_all[0].UT)\n",
    "# number of run\n",
    "nruns = len(ds_all[0].UC)\n",
    "verbose(2, \"%d subjects\" % len(ds_all))\n",
    "verbose(2, \"Per-subject dataset: %i samples with %i features\" % ds_all[0].shape)\n",
    "verbose(2, \"Stimulus categories: %s\" % ', '.join(ds_all[0].UT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVM, feature selection with one-way ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = LinearCSVMC()\n",
    "\n",
    "# feature selection helpers\n",
    "nf = 100\n",
    "fselector = FixedNElementTailSelector(nf, tail='upper',\n",
    "                                      mode='select', sort=False)\n",
    "sbfs = SensitivityBasedFeatureSelection(OneWayAnova(), fselector,\n",
    "                                        enable_ca=['sensitivities'])\n",
    "# create classifier with automatic feature selection\n",
    "fsclf = FeatureSelectionClassifier(clf, sbfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Within subject classification, leave one run out CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Performing classification analyses...\n",
      "  within-subject... done in 1.8 seconds\n"
     ]
    }
   ],
   "source": [
    "verbose(1, \"Performing classification analyses...\")\n",
    "verbose(2, \"within-subject...\", cr=False, lf=False)\n",
    "wsc_start_time = time.time()\n",
    "cv = CrossValidation(fsclf,\n",
    "                     NFoldPartitioner(attr='chunks'),\n",
    "                     errorfx=mean_match_accuracy)\n",
    "# store results in a sequence\n",
    "wsc_results = [cv(sd) for sd in ds_all]\n",
    "wsc_results = vstack(wsc_results)\n",
    "verbose(2, \" done in %.1f seconds\" % (time.time() - wsc_start_time,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Between subject classification, anatomically aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  between-subject (anatomically aligned)...done in 0.9 seconds\n"
     ]
    }
   ],
   "source": [
    "verbose(2, \"between-subject (anatomically aligned)...\", cr=False, lf=False)\n",
    "ds_mni = vstack(ds_all)\n",
    "mni_start_time = time.time()\n",
    "cv = CrossValidation(fsclf,\n",
    "                     NFoldPartitioner(attr='subject'),\n",
    "                     errorfx=mean_match_accuracy)\n",
    "bsc_mni_results = cv(ds_mni)\n",
    "verbose(2, \"done in %.1f seconds\" % (time.time() - mni_start_time,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperalignment!!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  between-subject (hyperaligned)...done in 3.9 seconds\n"
     ]
    }
   ],
   "source": [
    "verbose(2, \"between-subject (hyperaligned)...\", cr=False, lf=False)\n",
    "hyper_start_time = time.time()\n",
    "bsc_hyper_results = []\n",
    "# same cross-validation over subjects as before\n",
    "cv = CrossValidation(clf, NFoldPartitioner(attr='subject'),\n",
    "                     errorfx=mean_match_accuracy)\n",
    "\n",
    "# leave-one-run-out for hyperalignment training\n",
    "for test_run in range(nruns):\n",
    "    # split in training and testing set\n",
    "    ds_train = [sd[sd.sa.chunks != test_run, :] for sd in ds_all]\n",
    "    ds_test = [sd[sd.sa.chunks == test_run, :] for sd in ds_all]\n",
    "\n",
    "    # manual feature selection for every individual dataset in the list\n",
    "    anova = OneWayAnova()\n",
    "    fscores = [anova(sd) for sd in ds_train]\n",
    "    featsels = [StaticFeatureSelection(fselector(fscore)) for fscore in fscores]\n",
    "    ds_train_fs = [fs.forward(sd) for fs, sd in zip(featsels, ds_train)]\n",
    "\n",
    "\n",
    "    # Perform hyperalignment on the training data with default parameters.\n",
    "    # Computing hyperalignment parameters is as simple as calling the\n",
    "    # hyperalignment object with a list of datasets. All datasets must have the\n",
    "    # same number of samples and time-locked responses are assumed.\n",
    "    # Hyperalignment returns a list of mappers corresponding to subjects in the\n",
    "    # same order as the list of datasets we passed in.\n",
    "\n",
    "\n",
    "    hyper = Hyperalignment()\n",
    "    hypmaps = hyper(ds_train_fs)\n",
    "\n",
    "    # Applying hyperalignment parameters is similar to applying any mapper in\n",
    "    # PyMVPA. We start by selecting the voxels that we used to derive the\n",
    "    # hyperalignment parameters. And then apply the hyperalignment parameters\n",
    "    # by running the test dataset through the forward() function of the mapper.\n",
    "\n",
    "    ds_test_fs = [fs.forward(sd) for fs, sd in zip(featsels, ds_test)]\n",
    "    ds_hyper = [h.forward(sd) for h, sd in zip(hypmaps, ds_test_fs)]\n",
    "\n",
    "    # Now, we have a list of datasets with feature correspondence in a common\n",
    "    # space derived from the training data. Just as in the between-subject\n",
    "    # analyses of anatomically aligned data we can stack them all up and run the\n",
    "    # crossvalidation analysis.\n",
    "\n",
    "    ds_hyper = vstack(ds_hyper)\n",
    "    # zscore each subject individually after transformation for optimal\n",
    "    # performance\n",
    "    zscore(ds_hyper, chunks_attr='subject')\n",
    "    res_cv = cv(ds_hyper)\n",
    "    bsc_hyper_results.append(res_cv)\n",
    "\n",
    "bsc_hyper_results = hstack(bsc_hyper_results)\n",
    "verbose(2, \"done in %.1f seconds\" % (time.time() - hyper_start_time,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Average classification accuracies:\n",
      "  within-subject: 0.57 +/-0.063\n",
      "  between-subject (anatomically aligned): 0.42 +/-0.035\n",
      "  between-subject (hyperaligned): 0.62 +/-0.051\n"
     ]
    }
   ],
   "source": [
    "verbose(1, \"Average classification accuracies:\")\n",
    "verbose(2, \"within-subject: %.2f +/-%.3f\"\n",
    "        % (np.mean(wsc_results),\n",
    "           np.std(wsc_results) / np.sqrt(nsubjs - 1)))\n",
    "verbose(2, \"between-subject (anatomically aligned): %.2f +/-%.3f\"\n",
    "        % (np.mean(bsc_mni_results),\n",
    "           np.std(np.mean(bsc_mni_results, axis=1)) / np.sqrt(nsubjs - 1)))\n",
    "verbose(2, \"between-subject (hyperaligned): %.2f +/-%.3f\" \\\n",
    "        % (np.mean(bsc_hyper_results),\n",
    "           np.std(np.mean(bsc_hyper_results, axis=1)) / np.sqrt(nsubjs - 1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
